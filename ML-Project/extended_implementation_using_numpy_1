{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extended_implementation_using_numpy_1","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Y9azxIFa0org","colab_type":"text"},"source":["# ML Final Project\n"]},{"cell_type":"markdown","metadata":{"id":"GoDzUUtV0oy9","colab_type":"text"},"source":["# Extension 1\n"]},{"cell_type":"markdown","metadata":{"id":"96yBZd3PaO_6","colab_type":"text"},"source":["Naive Implementation on i, WITHOUT LASSO"]},{"cell_type":"code","metadata":{"id":"g_WamOpuOCkZ","colab_type":"code","outputId":"294329e3-79e1-4b2c-9b8c-28d13e85c1b7","executionInfo":{"status":"ok","timestamp":1589089164728,"user_tz":240,"elapsed":1177,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.datasets import load_breast_cancer # breast cancerin scikit learn data set\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n","from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n","from sklearn.metrics import accuracy_score # \n","import numpy as np\n","import numpy.random as r # We will randomly initialize our weights\n","import matplotlib.pyplot as plt \n","\n","breast_cancer=load_breast_cancer()\n","X = breast_cancer.data\n","print(\"The shape of the breast cancer dataset:\") \n","print(breast_cancer.data.shape)\n","y = breast_cancer.target\n","\n","X_scale = StandardScaler()\n","X = X_scale.fit_transform(breast_cancer.data)\n","\n","#Split the data into training and test set.  60% training and %40 test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9)\n","\n","scaler = preprocessing.StandardScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","one_col = np.ones((X_train.shape[0],1))\n","X_train = np.hstack((one_col, X_train ))\n","one_col = np.ones((X_test.shape[0],1))\n","X_test  = np.hstack((one_col, X_test ))\n","one_col = np.ones((X.shape[0],1))\n","X = np.hstack((one_col, X ))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The shape of the breast cancer dataset:\n","(569, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U3iY2mWqaRCj","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    sigmoid = 1/(1 + np.exp(-z)) \n","    return sigmoid"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UCFS-rBcAr5","colab_type":"code","outputId":"e2a035d4-20b8-4379-8f61-31b0b674379f","executionInfo":{"status":"ok","timestamp":1589089165276,"user_tz":240,"elapsed":999,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Initialize parameters w\n","w = np.zeros((X_train.shape[1], 1))\n","print(w.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(31, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"19ILsycwcEfP","colab_type":"code","colab":{}},"source":["# predict the probability that a patient has cancer \n","# TODO - Write the hypothesis function \n","def hypothesis(X , w):\n","    return sigmoid(np.dot(X, w))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwkQU5PacMI4","colab_type":"code","colab":{}},"source":["def log_likelihood(X , y , w ):\n","    ##TODO\n","    h_x = hypothesis(X, w)\n","    y_t = np.transpose(y)\n","    log_likelihood = np.dot(y_t, np.log(h_x)) + np.dot(1 - y_t, np.log(1 - h_x))\n","    ##\n","    return log_likelihood[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"paQXF0KuchbR","colab_type":"code","colab":{}},"source":["def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters):\n","    # For every 100 iterations, store the log_likelihood for the current w\n","    # Initializing log_likelihood to be an empty list  \n","    log_likelihood_values = []\n","    # Initialize w to be a zero vector of shape x_train.shape[1],1\n","    w = np.zeros((X.shape[1], 1))\n","    # Initialize N to the number of training examples\n","    N = X.shape[0] \n","    y = y.reshape(-1,1)\n","    ## TODO \n","    for i in range(num_iters):\n","        h_x = hypothesis(X,w)\n","        X_T = np.transpose(X)\n","        w += (learning_rate / N) * np.dot(X_T,(y - h_x)) \n","    ##\n","        if (i % 100) == 0:\n","            log_likelihood_values.append(log_likelihood(X,y,w))\n","        \n","    return w, log_likelihood_values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTBBLF1vcpAc","colab_type":"code","outputId":"d9b56457-30b7-48fc-b413-823dff31f35b","executionInfo":{"status":"ok","timestamp":1589089167485,"user_tz":240,"elapsed":1421,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["learning_rate = 0.5\n","num_iters = 5000 # The number of iteratins to run the gradient ascent algorithm\n","\n","w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n","\n","# Run this cell to plot Likelihood v/s Number of Iterations.\n","iters = np.array(range(0,num_iters,100))\n","plt.plot(iters,log_likelihood_values,'.-',color='green')\n","plt.xlabel('Number of iterations')\n","plt.ylabel('Likelihood')\n","plt.title(\"Likelihood vs Number of Iterations.\")\n","plt.grid()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"2ZG53cCrcwoR","colab_type":"code","colab":{}},"source":["def classify(w_Tx, threshold = 0.5):\n","    classifications = []\n","    for i in w_Tx:\n","        if i >= threshold:\n","            classifications.append(1)\n","        else:\n","            classifications.append(0)\n","    return np.array(classifications)\n","\n","predictions = classify(hypothesis(X_test, w))\n","predictions = predictions.reshape(-1,1)\n","y_test = y_test.reshape(-1,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZ0L-paYdKQH","colab_type":"code","outputId":"bbf32103-81b1-447f-fb8e-a8f45db476d1","executionInfo":{"status":"ok","timestamp":1589089169940,"user_tz":240,"elapsed":855,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","accuracy = (1 - error[0]) * 100\n","print(f\"The accuracy of logistic regression on breast canc dataset w/o regularization is {accuracy}%\")\n","print(w.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The accuracy of logistic regression on breast canc dataset w/o regularization is 93.17738791423002%\n","(31, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kh2vVT7pONBN","colab_type":"code","outputId":"99c8f509-93eb-44a3-ac29-b18629a14200","executionInfo":{"status":"ok","timestamp":1589089184006,"user_tz":240,"elapsed":12461,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["def hyperparameter_analysis(X, y, num_iters):\n","    optimal_accuracy = 0\n","    \n","    #hyperparameters\n","    optimal_threshold = 0\n","    optimal_LR = 0\n","    optimal_num_iters = 0\n","    \n","    N = len(X)\n","    y = y.reshape(-1,1)\n","    \n","    test_alphas = [0.01, 0.1, 0.5]\n","    test_thresholds = [0.3,0.5,0.7]\n","    \n","    for alpha in test_alphas:\n","        for threshold in test_thresholds:\n","            w = np.zeros((X_train.shape[1], 1))\n","            for i in range(num_iters):\n","                h_x = hypothesis(X, w)\n","                w += (alpha / N) * np.dot(X.T, y - h_x)\n","                \n","                #valdiate against test set\n","                predictions = classify(hypothesis(X_test, w),threshold).reshape(-1,1)\n","                error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","                accuracy = (1 - error[0]) * 100\n","                \n","                if (accuracy > optimal_accuracy):\n","                    optimal_threshold = threshold\n","                    optimal_LR = alpha\n","                    optimal_num_iters = i + 1\n","                    optimal_accuracy = accuracy\n","    \n","    print(f\"Accuracy: {optimal_accuracy} %\")\n","    print(f\"Optimal learning_rate = {optimal_LR}\")\n","    print(f\"Optimal threshold = {optimal_threshold}\")\n","    print(f\"Optimal # of iterations = {optimal_num_iters}\")\n","    \n","    #print(f\"The accuracy by these hyperparamters was {optimal_accuracy}!\")\n","\n","hyperparameter_analysis(X_train, y_train, 1000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy: 94.73684210526316 %\n","Optimal learning_rate = 0.01\n","Optimal threshold = 0.5\n","Optimal # of iterations = 632\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JKmSKLqHqv7e","colab_type":"text"},"source":["Naive EXTENDED algorithm on dataset in (i), breast cancer: Logistic Regression + LASSO"]},{"cell_type":"code","metadata":{"id":"_iRyVNhupA0U","colab_type":"code","outputId":"3741db77-978a-4a10-c43f-2cf09d2e2e38","executionInfo":{"status":"ok","timestamp":1589089184789,"user_tz":240,"elapsed":774,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["w = np.zeros((X_train.shape[1], 1))\n","print(w.shape)\n","\n","def Logistic_Regresion_Gradient_Ascent_regz(X, y, learning_rate, num_iters, penalty = 0.001):\n","    # For every 100 iterations, store the log_likelihood for the current w\n","    # Initializing log_likelihood to be an empty list  \n","    log_likelihood_values = []\n","    # Initialize w to be a zero vector of shape x_train.shape[1],1\n","    w = np.zeros((X.shape[1], 1))\n","    # Initialize N to the number of training examples\n","    N = X.shape[0] \n","    y = y.reshape(-1,1)\n","    ## TODO \n","    for i in range(num_iters):\n","        h_x = hypothesis(X,w)\n","        X_T = np.transpose(X)\n","        #This is the extension: occurs in weight update\n","        #First, update without worrying about penalty\n","        #Then, 'clip' regularization term so that regularization gradient will not change the sign of a parameter\n","        w += (learning_rate / N) * np.dot(X_T,(y - h_x)) #- (penalty * np.sign(w))\n","        for i in range(len(w)):\n","          if w[i] > 0:\n","            w[i] = max(0,w[i] - (penalty *learning_rate / N ))\n","          else:\n","            w[i] = min(0,w[i] + (penalty *learning_rate / N ))\n","    ##\n","        if (i % 100) == 0:\n","            log_likelihood_values.append(log_likelihood(X,y,w))\n","        \n","    return w, log_likelihood_values"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(31, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bL0Vv0WkpF4D","colab_type":"code","outputId":"8b9e7dd3-264b-47d5-95d9-6ae2b0a2d9e1","executionInfo":{"status":"ok","timestamp":1589089186016,"user_tz":240,"elapsed":1987,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["learning_rate = 0.5\n","num_iters = 5000 # The number of iteratins to run the gradient ascent algorithm\n","\n","w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent_regz(X_train, y_train, learning_rate, num_iters)\n","\n","print(w)\n","\n","print(w.shape)\n","\n","predictions = classify(hypothesis(X_test, w))\n","predictions = predictions.reshape(-1,1)\n","y_test = y_test.reshape(-1,1)\n","\n","error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","accuracy = (1 - error[0]) * 100\n","print(f\"The accuracy of logistic regression on breast cancer dataset w/ regularization is {accuracy}%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[-0.10401067]\n"," [-1.37714515]\n"," [-1.07268721]\n"," [-1.42771558]\n"," [-1.38266573]\n"," [-0.93873645]\n"," [-2.28564292]\n"," [-0.66358773]\n"," [-1.16573094]\n"," [-0.66293565]\n"," [ 0.47046256]\n"," [-2.510491  ]\n"," [ 0.973137  ]\n"," [-0.78679451]\n"," [-2.35842742]\n"," [ 1.51971325]\n"," [-0.33395316]\n"," [ 0.83595678]\n"," [ 1.81421431]\n"," [ 0.17080018]\n"," [ 0.52529079]\n"," [-1.93525393]\n"," [-0.40239738]\n"," [-1.54873297]\n"," [-1.65333745]\n"," [-1.66769481]\n"," [-1.69455888]\n"," [ 0.01395015]\n"," [-0.11892596]\n"," [-0.38431274]\n"," [-2.00883074]]\n","(31, 1)\n","The accuracy of logistic regression on breast cancer dataset w/ regularization is 93.17738791423002%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nlXNqyo9O1zd","colab_type":"code","colab":{}},"source":["w = np.zeros((X_train.shape[1], 1))\n","\n","def hyperparameter_analysis_2(X, y, num_iters, penalty):\n","    optimal_accuracy = 0\n","    \n","    #hyperparameters\n","    optimal_threshold = 0\n","    optimal_LR = 0\n","    optimal_num_iters = 0\n","    \n","    N = len(X)\n","    y = y.reshape(-1,1)\n","    \n","    test_alphas = [0.01, 0.1, 0.5]\n","    test_thresholds = [0.3,0.5,0.7]\n","    \n","    for alpha in test_alphas:\n","        for threshold in test_thresholds:\n","          w = np.zeros((X.shape[1], 1))\n","          for i in range(num_iters):\n","              h_x = hypothesis(X, w)\n","              X_T = np.transpose(X)\n","              w += (learning_rate / N) * np.dot(X_T,(y - h_x)) #- (penalty * np.sign(w))\n","              #w[w<0] = np.maximum(0, w[w<0] - (penalty *learning_rate / N))\n","              #w[w>0] = np.minimum(0,w[w>0] + (penalty *learning_rate / N))\n","              for i in range(len(w)):\n","                if w[i] > 0:\n","                  w[i] = max(0,w[i] + (penalty *learning_rate / N ))\n","                else:\n","                  w[i] = min(0,w[i] + (penalty *learning_rate / N ))\n","              \n","              #valdiate against test set\n","              predictions = classify(hypothesis(X_test, w),threshold).reshape(-1,1)\n","              error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","              accuracy = (1 - error[0]) * 100\n","              \n","              if (accuracy > optimal_accuracy):\n","                  optimal_threshold = threshold\n","                  optimal_LR = alpha\n","                  optimal_num_iters = i + 1\n","                  optimal_accuracy = accuracy\n","        \n","    print(f\"Optimal learning_rate = {optimal_LR}\")\n","    print(f\"Optimal threshold = {optimal_threshold}\")\n","    print(f\"Optimal # of iterations = {optimal_num_iters}\")\n","    print(f\"weights = {w}\")\n","    print(f\"Accuracy = {optimal_accuracy}!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfladCy_7386","colab_type":"code","outputId":"542f7ee7-b5d8-4a19-8f01-f5f03b13c2b5","executionInfo":{"status":"ok","timestamp":1589089257190,"user_tz":240,"elapsed":73145,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["test_penalties = [0.01, 0.1, 1, 2, 5]\n","\n","for penalty in test_penalties:\n","    print(f\"penalty = {penalty}:\")\n","    hyperparameter_analysis_2(X_train, y_train, 1000,penalty)\n","    print(\"\\n\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["penalty = 0.01:\n","Optimal learning_rate = 0.01\n","Optimal threshold = 0.3\n","Optimal # of iterations = 31\n","weights = [[ 0.20461979]\n"," [-0.99827948]\n"," [-0.65657795]\n"," [-1.03678508]\n"," [-1.00207119]\n"," [-0.65940406]\n"," [-1.55898362]\n"," [-0.5383452 ]\n"," [-0.96167014]\n"," [-0.47088886]\n"," [ 0.36168084]\n"," [-1.63031325]\n"," [ 0.714469  ]\n"," [-0.60248456]\n"," [-1.57159354]\n"," [ 1.02151016]\n"," [-0.2171514 ]\n"," [ 0.64631154]\n"," [ 1.12581019]\n"," [ 0.22445013]\n"," [ 0.41090341]\n"," [-1.32150311]\n"," [-0.26183839]\n"," [-1.1047305 ]\n"," [-1.14437215]\n"," [-1.05619246]\n"," [-1.15447966]\n"," [-0.03544657]\n"," [-0.2945412 ]\n"," [-0.29187183]\n"," [-1.28378116]]\n","Accuracy = 94.73684210526316!\n","\n","\n","penalty = 0.1:\n","Optimal learning_rate = 0.01\n","Optimal threshold = 0.3\n","Optimal # of iterations = 31\n","weights = [[ 1.00377823]\n"," [-0.49874581]\n"," [-0.47863449]\n"," [-0.58771095]\n"," [-0.51968986]\n"," [-0.4273466 ]\n"," [-1.76078111]\n"," [-0.53940899]\n"," [-0.87587144]\n"," [-0.57060674]\n"," [ 0.74607842]\n"," [-1.56283208]\n"," [ 0.93023629]\n"," [-0.48512585]\n"," [-1.42674032]\n"," [ 1.24220447]\n"," [-0.45119287]\n"," [ 0.87608049]\n"," [ 1.20180262]\n"," [ 0.25031667]\n"," [ 0.44468964]\n"," [-0.80063973]\n"," [ 0.        ]\n"," [-0.5966012 ]\n"," [-0.60910056]\n"," [-0.63456718]\n"," [-0.97096186]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.00476911]\n"," [-0.97769079]]\n","Accuracy = 94.9317738791423!\n","\n","\n","penalty = 1:\n","Optimal learning_rate = 0.01\n","Optimal threshold = 0.5\n","Optimal # of iterations = 31\n","weights = [[ 4.26047655]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-3.48431082]\n"," [-1.70094645]\n"," [-1.87890864]\n"," [-1.7021275 ]\n"," [ 4.38016137]\n"," [-2.02538837]\n"," [ 2.45018596]\n"," [-0.61693829]\n"," [-0.94531924]\n"," [ 0.90060661]\n"," [-0.37234236]\n"," [ 3.90804681]\n"," [ 0.        ]\n"," [ 1.37621776]\n"," [ 1.22886371]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.89834565]]\n","Accuracy = 94.54191033138402!\n","\n","\n","penalty = 2:\n","Optimal learning_rate = 0.01\n","Optimal threshold = 0.5\n","Optimal # of iterations = 31\n","weights = [[ 7.36349429]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-5.44386101]\n"," [-3.05510042]\n"," [-2.48489816]\n"," [-2.94383541]\n"," [ 7.7873305 ]\n"," [-2.92160552]\n"," [ 3.92586299]\n"," [-1.47323035]\n"," [ 0.        ]\n"," [ 0.91862967]\n"," [-0.73990257]\n"," [ 7.12265155]\n"," [ 0.        ]\n"," [ 2.34825676]\n"," [ 2.53661274]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.89828244]]\n","Accuracy = 93.76218323586744!\n","\n","\n","penalty = 5:\n","Optimal learning_rate = 0.01\n","Optimal threshold = 0.5\n","Optimal # of iterations = 31\n","weights = [[ 17.55412896]\n"," [  0.        ]\n"," [  0.        ]\n"," [  0.        ]\n"," [  0.        ]\n"," [  0.        ]\n"," [-12.18156153]\n"," [ -9.11200436]\n"," [ -2.71068192]\n"," [ -2.47669234]\n"," [ 17.7166081 ]\n"," [ -5.39475673]\n"," [ 15.15640778]\n"," [ -4.12800978]\n"," [  0.        ]\n"," [  2.83445467]\n"," [ -2.52091406]\n"," [ 15.94452449]\n"," [  0.        ]\n"," [  4.35080032]\n"," [  9.1990214 ]\n"," [  0.        ]\n"," [  0.        ]\n"," [  0.        ]\n"," [  0.        ]\n"," [  0.        ]\n"," [ -1.87333008]\n"," [  0.        ]\n"," [  0.        ]\n"," [  0.        ]\n"," [ -0.80850174]]\n","Accuracy = 92.39766081871346!\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nLh4o6rLPmWQ","colab_type":"text"},"source":["Naive Implementation on Wine, (ii) (Logistic WITHOUT LASSO)"]},{"cell_type":"code","metadata":{"id":"f5-RXqLCP4Fk","colab_type":"code","outputId":"7d45fd75-929e-4f6e-894d-b614d81978bc","executionInfo":{"status":"ok","timestamp":1588886675935,"user_tz":240,"elapsed":899,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.datasets import load_wine # wine in scikit learn data set\n","from sklearn import preprocessing\n","from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n","from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n","from sklearn.metrics import accuracy_score # \n","import numpy as np\n","import numpy.random as r # We will randomly initialize our weights\n","import matplotlib.pyplot as plt \n","\n","wine=load_wine()\n","X = wine.data\n","print(\"The shape of the wine dataset:\") \n","print(wine.data.shape)\n","y = wine.target\n","\n","X = np.vstack([X[y==1],X[y==2]])\n","y = np.hstack([y[y==1],y[y==2]])\n","\n","#Split the data into training and test set.  60% training and %40 test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n","\n","scaler = preprocessing.StandardScaler().fit(X_train)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","#for some reason, I got terrible results when I added the one's column\n","#like, ~60% accuracy \n","#so I just commented this out\n","#one_col = np.ones((X_train.shape[0],1))\n","#X_train = np.hstack((one_col, X_train ))\n","#one_col = np.ones((X_test.shape[0],1))\n","#X_test  = np.hstack((one_col, X_test ))\n","#one_col = np.ones((X.shape[0],1))\n","#X = np.hstack((one_col, X ))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The shape of the wine dataset:\n","(178, 13)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RFZ7XKvpQABy","colab_type":"code","outputId":"3cb3a4e3-cb6a-48bd-d23b-4c8737f7fb99","executionInfo":{"status":"ok","timestamp":1588886676755,"user_tz":240,"elapsed":1077,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["learning_rate = 0.5\n","num_iters = 5000 # The number of iteratins to run the gradient ascent algorithm\n","\n","w = np.zeros((X_train.shape[1], 1))\n","\n","w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n","\n","def classify(w_Tx, threshold = 0.5):\n","    classifications = []\n","    for i in w_Tx:\n","        if i >= threshold:\n","            classifications.append(2)\n","        else:\n","            classifications.append(1)\n","    return np.array(classifications)\n","\n","\n","predictions = classify(hypothesis(X_test, w))\n","predictions = predictions.reshape(-1,1)\n","y_test = y_test.reshape(-1,1)\n","\n","error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","accuracy = (1 - error[0]) * 100\n","print(f\"The accuracy of logistic regression on wine dataset w/o regularization is {accuracy}%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The accuracy of logistic regression on wine dataset w/o regularization is 93.75%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JECBc0ndQV9b","colab_type":"code","outputId":"abe78f25-f2a1-4e86-f6ad-caf5da8c08f0","executionInfo":{"status":"ok","timestamp":1588886676971,"user_tz":240,"elapsed":732,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["w = np.zeros((X_train.shape[1], 1))\n","\n","for learning_rate in [0.1, 0.5,0.7,0.9]:\n","  for threshold in [0.3,0.5,0.7]:\n","    print(f\"threshold = {threshold}, learning rate = {learning_rate}:\")\n","    w = np.zeros((X_train.shape[1], 1))\n","\n","    w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, 1000)\n","\n","    predictions = classify(hypothesis(X_test, w),threshold)\n","    predictions = predictions.reshape(-1,1)\n","    y_test = y_test.reshape(-1,1)\n","\n","    error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","    accuracy = (1 - error[0]) * 100\n","    print(f\"the accuracy by these hyperparameters is {accuracy}%\\n\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["threshold = 0.3, learning rate = 0.1:\n","the accuracy by these hyperparameters is 89.58333333333334%\n","\n","threshold = 0.5, learning rate = 0.1:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.7, learning rate = 0.1:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.3, learning rate = 0.5:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.5, learning rate = 0.5:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.7, learning rate = 0.5:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.3, learning rate = 0.7:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.5, learning rate = 0.7:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.7, learning rate = 0.7:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.3, learning rate = 0.9:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.5, learning rate = 0.9:\n","the accuracy by these hyperparameters is 93.75%\n","\n","threshold = 0.7, learning rate = 0.9:\n","the accuracy by these hyperparameters is 93.75%\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S-nDFj_HSoL6","colab_type":"code","outputId":"8e967389-b2cd-470c-ac65-63981c300f21","executionInfo":{"status":"ok","timestamp":1588886678155,"user_tz":240,"elapsed":1292,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["print(w)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ 0.80659824]\n"," [ 1.47155815]\n"," [ 1.61205654]\n"," [ 0.9681972 ]\n"," [ 0.05585362]\n"," [-0.08904403]\n"," [-2.63464258]\n"," [-1.26498313]\n"," [ 0.23024543]\n"," [ 3.83694117]\n"," [-2.18982331]\n"," [-1.94992077]\n"," [ 1.94220577]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"npa2vULSQVWE","colab_type":"text"},"source":["Naive EXTENDED implementation w wine (ii), Logistic Regression + LASSO regularization\n","\n"]},{"cell_type":"code","metadata":{"id":"iF6hrvV1QT3F","colab_type":"code","outputId":"11d402c4-c13a-45f2-f637-542009584a50","executionInfo":{"status":"ok","timestamp":1588886679456,"user_tz":240,"elapsed":1366,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["w = np.zeros((X_train.shape[1], 1))\n","\n","w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent_regz(X_train, y_train, learning_rate, num_iters)\n","\n","predictions = classify(hypothesis(X_test, w))\n","predictions = predictions.reshape(-1,1)\n","y_test = y_test.reshape(-1,1)\n","\n","error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","accuracy = (1 - error[0]) * 100\n","print(f\"The accuracy of logistic regression on wine dataset w/ regularization is {accuracy}%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The accuracy of logistic regression on wine dataset w/ regularization is 93.75%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s-IvYL0lQdfd","colab_type":"code","outputId":"40150b30-28b8-4e22-ec80-6ae74ef2df9c","executionInfo":{"status":"ok","timestamp":1588886684537,"user_tz":240,"elapsed":5914,"user":{"displayName":"Indu Ramesh","photoUrl":"","userId":"08622647505491358185"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["test_penalties = [0.01, 0.1, 1, 2, 5]\n","for learning_rate in [0.5,0.7,0.9]:\n","  for threshold in [0.3,0.5,0.7]:\n","    for penalty in test_penalties:\n","        print(f\"threshold = {threshold}, learning rate = {learning_rate}, penalty = {penalty}:\")\n","        #hyperparameter_analysis_2(X, y, 1000,penalty, mixing)\n","        #w = np.zeros((X_train.shape[1], 1))\n","\n","        w, log_likelihood_values = Logistic_Regresion_Gradient_Ascent_regz(X_train, y_train, learning_rate, 1000, penalty)\n","\n","        predictions = classify(hypothesis(X_test, w))\n","        predictions = predictions.reshape(-1,1)\n","        y_test = y_test.reshape(-1,1)\n","\n","        error = [np.count_nonzero(predictions - y_test) / len(y_test)] \n","        accuracy = (1 - error[0]) * 100\n","        print(f\"Accuracy of logistic regression on wine dataset w/ LASSO regularization is {accuracy}%\")\n","        print(f\"weights = {w}\")\n","        print(\"\\n\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["threshold = 0.3, learning rate = 0.5, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.77633866]\n"," [ 1.26507268]\n"," [ 1.35863947]\n"," [ 0.84932736]\n"," [ 0.08440913]\n"," [-0.09077638]\n"," [-2.25363673]\n"," [-1.037595  ]\n"," [ 0.14738348]\n"," [ 3.28665185]\n"," [-1.92686473]\n"," [-1.69557066]\n"," [ 1.67235516]]\n","\n","\n","threshold = 0.3, learning rate = 0.5, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.51576183]\n"," [ 0.9573512 ]\n"," [ 1.06720216]\n"," [ 0.63458073]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.92844638]\n"," [-0.892949  ]\n"," [ 0.        ]\n"," [ 3.03948952]\n"," [-1.64241335]\n"," [-1.28100066]\n"," [ 1.47937053]]\n","\n","\n","threshold = 0.3, learning rate = 0.5, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.22801068]\n"," [ 0.32300107]\n"," [ 0.43107759]\n"," [ 0.11483266]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.14962912]\n"," [-0.1065309 ]\n"," [ 0.        ]\n"," [ 2.1320819 ]\n"," [-0.92367806]\n"," [-0.43514074]\n"," [ 0.50173248]]\n","\n","\n","threshold = 0.3, learning rate = 0.5, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09415723]\n"," [ 0.23776728]\n"," [ 0.29354336]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.95845042]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.77496089]\n"," [-0.62646242]\n"," [-0.39640343]\n"," [ 0.16180071]]\n","\n","\n","threshold = 0.3, learning rate = 0.5, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11342786]\n"," [ 0.04860957]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68488957]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.2086462 ]\n"," [-0.31413029]\n"," [-0.39913358]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.5, learning rate = 0.5, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.77633866]\n"," [ 1.26507268]\n"," [ 1.35863947]\n"," [ 0.84932736]\n"," [ 0.08440913]\n"," [-0.09077638]\n"," [-2.25363673]\n"," [-1.037595  ]\n"," [ 0.14738348]\n"," [ 3.28665185]\n"," [-1.92686473]\n"," [-1.69557066]\n"," [ 1.67235516]]\n","\n","\n","threshold = 0.5, learning rate = 0.5, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.51576183]\n"," [ 0.9573512 ]\n"," [ 1.06720216]\n"," [ 0.63458073]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.92844638]\n"," [-0.892949  ]\n"," [ 0.        ]\n"," [ 3.03948952]\n"," [-1.64241335]\n"," [-1.28100066]\n"," [ 1.47937053]]\n","\n","\n","threshold = 0.5, learning rate = 0.5, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.22801068]\n"," [ 0.32300107]\n"," [ 0.43107759]\n"," [ 0.11483266]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.14962912]\n"," [-0.1065309 ]\n"," [ 0.        ]\n"," [ 2.1320819 ]\n"," [-0.92367806]\n"," [-0.43514074]\n"," [ 0.50173248]]\n","\n","\n","threshold = 0.5, learning rate = 0.5, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09415723]\n"," [ 0.23776728]\n"," [ 0.29354336]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.95845042]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.77496089]\n"," [-0.62646242]\n"," [-0.39640343]\n"," [ 0.16180071]]\n","\n","\n","threshold = 0.5, learning rate = 0.5, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11342786]\n"," [ 0.04860957]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68488957]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.2086462 ]\n"," [-0.31413029]\n"," [-0.39913358]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.7, learning rate = 0.5, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.77633866]\n"," [ 1.26507268]\n"," [ 1.35863947]\n"," [ 0.84932736]\n"," [ 0.08440913]\n"," [-0.09077638]\n"," [-2.25363673]\n"," [-1.037595  ]\n"," [ 0.14738348]\n"," [ 3.28665185]\n"," [-1.92686473]\n"," [-1.69557066]\n"," [ 1.67235516]]\n","\n","\n","threshold = 0.7, learning rate = 0.5, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.51576183]\n"," [ 0.9573512 ]\n"," [ 1.06720216]\n"," [ 0.63458073]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.92844638]\n"," [-0.892949  ]\n"," [ 0.        ]\n"," [ 3.03948952]\n"," [-1.64241335]\n"," [-1.28100066]\n"," [ 1.47937053]]\n","\n","\n","threshold = 0.7, learning rate = 0.5, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.22801068]\n"," [ 0.32300107]\n"," [ 0.43107759]\n"," [ 0.11483266]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.14962912]\n"," [-0.1065309 ]\n"," [ 0.        ]\n"," [ 2.1320819 ]\n"," [-0.92367806]\n"," [-0.43514074]\n"," [ 0.50173248]]\n","\n","\n","threshold = 0.7, learning rate = 0.5, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09415723]\n"," [ 0.23776728]\n"," [ 0.29354336]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.95845042]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.77496089]\n"," [-0.62646242]\n"," [-0.39640343]\n"," [ 0.16180071]]\n","\n","\n","threshold = 0.7, learning rate = 0.5, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11342786]\n"," [ 0.04860957]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68488957]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.2086462 ]\n"," [-0.31413029]\n"," [-0.39913358]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.3, learning rate = 0.7, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.75795298]\n"," [ 1.33738803]\n"," [ 1.46661878]\n"," [ 0.88569752]\n"," [ 0.04711308]\n"," [-0.03745033]\n"," [-2.43214031]\n"," [-1.14823292]\n"," [ 0.17136125]\n"," [ 3.56552189]\n"," [-2.03753604]\n"," [-1.79000453]\n"," [ 1.80292203]]\n","\n","\n","threshold = 0.3, learning rate = 0.7, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.4326846 ]\n"," [ 0.96384558]\n"," [ 1.07983821]\n"," [ 0.62872114]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-2.0074176 ]\n"," [-0.96021321]\n"," [ 0.        ]\n"," [ 3.26371074]\n"," [-1.67006253]\n"," [-1.23107255]\n"," [ 1.51199803]]\n","\n","\n","threshold = 0.3, learning rate = 0.7, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.21410294]\n"," [ 0.3210168 ]\n"," [ 0.44059462]\n"," [ 0.10797612]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.18341161]\n"," [-0.11973255]\n"," [ 0.        ]\n"," [ 2.16734965]\n"," [-0.90853856]\n"," [-0.41115058]\n"," [ 0.48577898]]\n","\n","\n","threshold = 0.3, learning rate = 0.7, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09165503]\n"," [ 0.2372719 ]\n"," [ 0.29355535]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.96336681]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.78161957]\n"," [-0.62483461]\n"," [-0.39241136]\n"," [ 0.15937396]]\n","\n","\n","threshold = 0.3, learning rate = 0.7, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11343156]\n"," [ 0.04860654]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68490529]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.20866864]\n"," [-0.31411805]\n"," [-0.39911211]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.5, learning rate = 0.7, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.75795298]\n"," [ 1.33738803]\n"," [ 1.46661878]\n"," [ 0.88569752]\n"," [ 0.04711308]\n"," [-0.03745033]\n"," [-2.43214031]\n"," [-1.14823292]\n"," [ 0.17136125]\n"," [ 3.56552189]\n"," [-2.03753604]\n"," [-1.79000453]\n"," [ 1.80292203]]\n","\n","\n","threshold = 0.5, learning rate = 0.7, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.4326846 ]\n"," [ 0.96384558]\n"," [ 1.07983821]\n"," [ 0.62872114]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-2.0074176 ]\n"," [-0.96021321]\n"," [ 0.        ]\n"," [ 3.26371074]\n"," [-1.67006253]\n"," [-1.23107255]\n"," [ 1.51199803]]\n","\n","\n","threshold = 0.5, learning rate = 0.7, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.21410294]\n"," [ 0.3210168 ]\n"," [ 0.44059462]\n"," [ 0.10797612]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.18341161]\n"," [-0.11973255]\n"," [ 0.        ]\n"," [ 2.16734965]\n"," [-0.90853856]\n"," [-0.41115058]\n"," [ 0.48577898]]\n","\n","\n","threshold = 0.5, learning rate = 0.7, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09165503]\n"," [ 0.2372719 ]\n"," [ 0.29355535]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.96336681]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.78161957]\n"," [-0.62483461]\n"," [-0.39241136]\n"," [ 0.15937396]]\n","\n","\n","threshold = 0.5, learning rate = 0.7, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11343156]\n"," [ 0.04860654]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68490529]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.20866864]\n"," [-0.31411805]\n"," [-0.39911211]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.7, learning rate = 0.7, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.75795298]\n"," [ 1.33738803]\n"," [ 1.46661878]\n"," [ 0.88569752]\n"," [ 0.04711308]\n"," [-0.03745033]\n"," [-2.43214031]\n"," [-1.14823292]\n"," [ 0.17136125]\n"," [ 3.56552189]\n"," [-2.03753604]\n"," [-1.79000453]\n"," [ 1.80292203]]\n","\n","\n","threshold = 0.7, learning rate = 0.7, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.4326846 ]\n"," [ 0.96384558]\n"," [ 1.07983821]\n"," [ 0.62872114]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-2.0074176 ]\n"," [-0.96021321]\n"," [ 0.        ]\n"," [ 3.26371074]\n"," [-1.67006253]\n"," [-1.23107255]\n"," [ 1.51199803]]\n","\n","\n","threshold = 0.7, learning rate = 0.7, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.21410294]\n"," [ 0.3210168 ]\n"," [ 0.44059462]\n"," [ 0.10797612]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.18341161]\n"," [-0.11973255]\n"," [ 0.        ]\n"," [ 2.16734965]\n"," [-0.90853856]\n"," [-0.41115058]\n"," [ 0.48577898]]\n","\n","\n","threshold = 0.7, learning rate = 0.7, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09165503]\n"," [ 0.2372719 ]\n"," [ 0.29355535]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.96336681]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.78161957]\n"," [-0.62483461]\n"," [-0.39241136]\n"," [ 0.15937396]]\n","\n","\n","threshold = 0.7, learning rate = 0.7, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11343156]\n"," [ 0.04860654]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68490529]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.20866864]\n"," [-0.31411805]\n"," [-0.39911211]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.3, learning rate = 0.9, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.74167591]\n"," [ 1.38885333]\n"," [ 1.54504346]\n"," [ 0.91066566]\n"," [ 0.01841598]\n"," [ 0.        ]\n"," [-2.56386123]\n"," [-1.23092338]\n"," [ 0.1866179 ]\n"," [ 3.77409965]\n"," [-2.11833996]\n"," [-1.85723589]\n"," [ 1.89958614]]\n","\n","\n","threshold = 0.3, learning rate = 0.9, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.36865268]\n"," [ 0.95964611]\n"," [ 1.0786147 ]\n"," [ 0.62171248]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-2.06257458]\n"," [-1.00105797]\n"," [ 0.        ]\n"," [ 3.42917273]\n"," [-1.67619601]\n"," [-1.17354674]\n"," [ 1.51510513]]\n","\n","\n","threshold = 0.3, learning rate = 0.9, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.20848516]\n"," [ 0.32050174]\n"," [ 0.44446385]\n"," [ 0.10553312]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.19787012]\n"," [-0.12552641]\n"," [ 0.        ]\n"," [ 2.1805759 ]\n"," [-0.90154594]\n"," [-0.40184161]\n"," [ 0.47891679]]\n","\n","\n","threshold = 0.3, learning rate = 0.9, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09124234]\n"," [ 0.23717847]\n"," [ 0.29356293]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.96419468]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.78270266]\n"," [-0.62458505]\n"," [-0.39174286]\n"," [ 0.15897214]]\n","\n","\n","threshold = 0.3, learning rate = 0.9, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11343163]\n"," [ 0.04860648]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68490559]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.20866906]\n"," [-0.31411782]\n"," [-0.3991117 ]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.5, learning rate = 0.9, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.74167591]\n"," [ 1.38885333]\n"," [ 1.54504346]\n"," [ 0.91066566]\n"," [ 0.01841598]\n"," [ 0.        ]\n"," [-2.56386123]\n"," [-1.23092338]\n"," [ 0.1866179 ]\n"," [ 3.77409965]\n"," [-2.11833996]\n"," [-1.85723589]\n"," [ 1.89958614]]\n","\n","\n","threshold = 0.5, learning rate = 0.9, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.36865268]\n"," [ 0.95964611]\n"," [ 1.0786147 ]\n"," [ 0.62171248]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-2.06257458]\n"," [-1.00105797]\n"," [ 0.        ]\n"," [ 3.42917273]\n"," [-1.67619601]\n"," [-1.17354674]\n"," [ 1.51510513]]\n","\n","\n","threshold = 0.5, learning rate = 0.9, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.20848516]\n"," [ 0.32050174]\n"," [ 0.44446385]\n"," [ 0.10553312]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.19787012]\n"," [-0.12552641]\n"," [ 0.        ]\n"," [ 2.1805759 ]\n"," [-0.90154594]\n"," [-0.40184161]\n"," [ 0.47891679]]\n","\n","\n","threshold = 0.5, learning rate = 0.9, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09124234]\n"," [ 0.23717847]\n"," [ 0.29356293]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.96419468]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.78270266]\n"," [-0.62458505]\n"," [-0.39174286]\n"," [ 0.15897214]]\n","\n","\n","threshold = 0.5, learning rate = 0.9, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11343163]\n"," [ 0.04860648]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68490559]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.20866906]\n"," [-0.31411782]\n"," [-0.3991117 ]\n"," [ 0.        ]]\n","\n","\n","threshold = 0.7, learning rate = 0.9, penalty = 0.01:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.74167591]\n"," [ 1.38885333]\n"," [ 1.54504346]\n"," [ 0.91066566]\n"," [ 0.01841598]\n"," [ 0.        ]\n"," [-2.56386123]\n"," [-1.23092338]\n"," [ 0.1866179 ]\n"," [ 3.77409965]\n"," [-2.11833996]\n"," [-1.85723589]\n"," [ 1.89958614]]\n","\n","\n","threshold = 0.7, learning rate = 0.9, penalty = 0.1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 93.75%\n","weights = [[ 0.36865268]\n"," [ 0.95964611]\n"," [ 1.0786147 ]\n"," [ 0.62171248]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-2.06257458]\n"," [-1.00105797]\n"," [ 0.        ]\n"," [ 3.42917273]\n"," [-1.67619601]\n"," [-1.17354674]\n"," [ 1.51510513]]\n","\n","\n","threshold = 0.7, learning rate = 0.9, penalty = 1:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 95.83333333333334%\n","weights = [[ 0.20848516]\n"," [ 0.32050174]\n"," [ 0.44446385]\n"," [ 0.10553312]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-1.19787012]\n"," [-0.12552641]\n"," [ 0.        ]\n"," [ 2.1805759 ]\n"," [-0.90154594]\n"," [-0.40184161]\n"," [ 0.47891679]]\n","\n","\n","threshold = 0.7, learning rate = 0.9, penalty = 2:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.09124234]\n"," [ 0.23717847]\n"," [ 0.29356293]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.96419468]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.78270266]\n"," [-0.62458505]\n"," [-0.39174286]\n"," [ 0.15897214]]\n","\n","\n","threshold = 0.7, learning rate = 0.9, penalty = 5:\n","Accuracy of logistic regression on wine dataset w/ LASSO regularization is 97.91666666666666%\n","weights = [[ 0.        ]\n"," [ 0.11343163]\n"," [ 0.04860648]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [-0.68490559]\n"," [ 0.        ]\n"," [ 0.        ]\n"," [ 1.20866906]\n"," [-0.31411782]\n"," [-0.3991117 ]\n"," [ 0.        ]]\n","\n","\n"],"name":"stdout"}]}]}